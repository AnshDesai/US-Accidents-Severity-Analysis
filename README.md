US Accidents Severity Analysis
============

Stream producer for time series data using Kafka.

The producer and consumer Python scripts use [Confluent's Kafka client for Python](https://github.com/confluentinc/confluent-kafka-python), which is installed in the Docker image built with the accompanying Dockerfile, if you choose to use it.

Requires Docker and Docker Compose.

Usage
-------------------

Clone repo and cd into directory.

```
git clone https://github.com/AnshDesai/US-Accidents-Severity-Analysis.git
cd time-series-kafka-demo
```

**Start the Kafka broker**

```
docker compose up --build
```

**Produce a time series stream**

Send time series from data/data.csv to topic “my-stream”, and speed it up by a factor of 10.

```
python sendstream.py data/testdata.csv accidents-stream --speed 10
```

or with Docker:

```
docker run -it --rm \
      -v $PWD:/home \
      --network=host \
      kafkacsv python bin/sendstream.py data/testdata.csv accidents-stream --speed 10
```

When complete:
**Shut down and clean up**

Stop the consumer with Return and Ctrl+C.

Shutdown Kafka broker system:

```
docker compose down
```



# Real-time US Accidents Data Analysis

This is Python based Exploratory Data Analysis on "US Accidents" dataset to find out and visualize different trends including impact of COVID-19 on traffic behavior and accidents, environmental stimuli on accident occurrence and accident hotspot locations in order to get control over the same.

This is a countrywide traffic accident dataset, which covers 49 states of the United States. The data is continuously being collected from February 2016, using several data providers, including two APIs which provide streaming traffic event data. These APIs broadcast traffic events captured by a variety of entities, such as the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road-networks. Currently, there are about 3 million accident--updated records in this dataset.


## Table of contents
- [Architecture](#Architecture)
- [About the Project](#about-the-project)
  * [Tech Stack](#tech-stack)
  * [Environment Variables](#environment-variables)
- [Getting Started](#getting-started)
  * [Prerequisites](#prerequisites)
  * [Installation](#installation)
  * [Run Locally](#running-tests)
- [Visualisation](#usage)
- [Contributing](#contributing)
  * [Code of Conduct](#code-of-conduct)
- [FAQ](#faq)
- [License](#license)
- [Contact](#contact)
- [Acknowledgements](#acknowledgements)

## Architecture
![alt text](https://github.com/AnshDesai/US-Accidents-Severity-Analysis/blob/master/architecture-image.png?raw=true)

## About the project
Streaming data integration is the foundation for leveraging streaming analytics. Specific use cases such as Fraud detection, contextual marketing triggers, Dynamic pricing all rely on leveraging a data feed or real-time data. In many data centers, different type of servers generate large amount of data(events, Event in this case is status of the server in the data center) in real-time. There is always a need to process these data in real-time and generate insights which will be used by the server/data center monitoring people and they have to track these server's status regularly and find the resolution in case of issues occurring, for better server stability. Since the data is huge and coming in real-time, we need to choose the right architecture with scalable storage and computation frameworks/technologies.
Hence we want to build the Real Time Data Pipeline Using AWS Sagemaker, Apache Kafka, AWS S3, Lambda, MongoDB, Flask to generate insights out of this data.

## Tech Stack

#### Source of data:
Streaming data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes). Streaming data includes a wide variety of data such as log files generated by customers using your mobile or web applications, ecommerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices or instrumentation in data centers.
Streaming data processing is beneficial in most scenarios where new, dynamic data is generated on a continual basis. It applies to most of the industry segments and big data use cases. Companies generally begin with simple applications such as collecting system logs and rudimentary processing like rolling min-max computations. Then, these applications evolve to more sophisticated near-real-time processing. Initially, applications may process data streams to produce simple reports, and perform simple actions in response, such as emitting alarms when key measures exceed certain thresholds. Eventually, those applications perform more sophisticated forms of data analysis, like applying machine learning algorithms, and extract deeper insights from the data. Over time, complex, stream and event processing algorithms, like decaying time windows to find the most recent popular movies, are applied, further enriching the insights.

Data Source
https://www.kaggle.com/sobhanmoosavi/us-accidents

#### Data Ingestion:
Since the data is huge and coming in real-time, we need to choose the right architecture with scalable storage and computation technologies.



#### Data processing:

#### Database:
* MongoDB
With MongoDB, applications do not need to create the database and collection before they start writing data. These objects are created automatically upon first arrival of data into MongoDB. However, a time-series collection type needs to be created first before you start writing data. To make it easy to ingest time-series data into MongoDB from Kafka, these collection options are exposed as sink parameters and the time-series collection is created by the connector if it doesn’t already exist .

## Environment Variables

`ENDPOINT_NAME`


### Prerequisites
* Python 3.7 or later
* Apache kafka
* Flask
* MongoDB

### Installation
* Run this command to install python pakages
    ```bash
    pip freeze > requirements.txt
    ```    

* Mongo DB
    
    1. Download and install latest [MongoDB Compass](https://www.mongodb.com/try/download/community).
    2. Create database and a collection that can be configured in consumer.py file.
     
## Running Tests

To run tests, run the following commands.
#### Ignore steps 1-3 if already completed while installation:
 
1. Create Kafka topic
  ```bash
  kafka-topics.bat --zookeeper 127.0.0.1:2181 --create --replication-factor 1 --partitions 1 --topic accidents-stream  
```
6. Run Producer (This will help data flow from kafka to spark streaming)
```bash 
python sendstream.py data/testdata.csv accidents-stream --speed 10
```
7. Run consumer (This will help data be processed and consumed by mongodb for storage)
```bash
python consumestream.py accidents-stream
```

## Visualisation
Examples of accidents hotspot:

![alt text](https://github.com/AnshDesai/US-Accidents-Severity-Analysis/blob/master/hotspot-image.png?raw=true)

## Contributing

Contributions are always welcome!
Please adhere to this project's `code of conduct`.


## FAQ

#### Can other big data tools work with this project?
Yes, some other big data tools that can be used are: 
* For data storage: Hive, cassandra etc.
* For visualisation: Plotly.js, Kibana(Using ElasticSearch)

#### Can this architecture be deployed on cloud?
Yes, this framework can be deployed on cloud (AWS, GoogleCloud etc.) 
* One can use Kinesis Firehose for data channel and S3 for data storage.

## License

[MIT](https://choosealicense.com/licenses/mit/)


## Authors

- [@AnshDesai](https://www.github.com/AnshDesai)


## Acknowledgements

 - [Real-Time Data Analytics Framework for Twitter Streaming Data](https://towardsdatascience.com/usa-accidents-data-analysis-d130843cde02)

## Feedback

If you have any feedback, please reach out at anshdesai20@gmail.com

